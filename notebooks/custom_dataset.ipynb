{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=5, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set_file = \"../customdata/Trainingpoints.txt\"\n",
    "\n",
    "with open(training_set_file) as f:\n",
    "    training_set = f.readlines()\n",
    "\n",
    "training_set = [tuple(line.strip().split(\",\")) for line in training_set]\n",
    "tp_to_i = {tp:i+1 for i, tp in enumerate(training_set)}\n",
    "i_to_tp = {i:tp for tp, i in tp_to_i.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "# get files \n",
    "files_dir = \"../customdata/TrainingSet/\"\n",
    "# get list of files\n",
    "training_files = [file for file in os.listdir(files_dir) if file.endswith(\".txt\")]\n",
    "training_files = sorted(training_files, key=lambda x: int(os.path.splitext(x)[0]))\n",
    "# list of dfs\n",
    "dfs = []\n",
    "for file in training_files:\n",
    "  file_path = os.path.join(files_dir, file)\n",
    "  id = os.path.splitext(file)[0]\n",
    "  # get coordinates of scan\n",
    "  coords = np.array(list(i_to_tp[int(id)]))\n",
    "\n",
    "  df = pd.read_csv(file_path, header=None, sep=\",\")\n",
    "  df.drop(df.columns[-1], axis=1, inplace=True)\n",
    "  df.columns = [f\"AP_{i}\" for i in range(1, len(df.columns)+1)]\n",
    "  tiled_coords = np.tile(coords, (len(df), 1))\n",
    "  df = pd.concat([df, pd.DataFrame(tiled_coords, columns=[\"x\", \"y\"])], axis=1)\n",
    "  dfs.append(df)\n",
    "\n",
    "data = pd.concat(dfs, ignore_index=True).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# denormalize rss values\n",
    "min_value = -100 # from paper\n",
    "max_value = -26 # manually searched for max value in dataset\n",
    "\n",
    "# denormalize all data except last 2 columns\n",
    "data.iloc[:, :-2] = data.iloc[:, :-2] * (max_value - min_value) + min_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AP_1</th>\n",
       "      <th>AP_2</th>\n",
       "      <th>AP_3</th>\n",
       "      <th>AP_4</th>\n",
       "      <th>AP_5</th>\n",
       "      <th>AP_6</th>\n",
       "      <th>AP_7</th>\n",
       "      <th>AP_8</th>\n",
       "      <th>AP_9</th>\n",
       "      <th>AP_10</th>\n",
       "      <th>...</th>\n",
       "      <th>AP_115</th>\n",
       "      <th>AP_116</th>\n",
       "      <th>AP_117</th>\n",
       "      <th>AP_118</th>\n",
       "      <th>AP_119</th>\n",
       "      <th>AP_120</th>\n",
       "      <th>AP_121</th>\n",
       "      <th>AP_122</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-74.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>1173</td>\n",
       "      <td>670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-74.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>1173</td>\n",
       "      <td>670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-70.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>1173</td>\n",
       "      <td>670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-70.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>1173</td>\n",
       "      <td>670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-66.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>1173</td>\n",
       "      <td>670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4717</th>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-71.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-84.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>413</td>\n",
       "      <td>1160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4718</th>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-76.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-84.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>413</td>\n",
       "      <td>1160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4719</th>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-76.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-84.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>413</td>\n",
       "      <td>1160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4720</th>\n",
       "      <td>-66.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-77.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-87.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>413</td>\n",
       "      <td>1160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4721</th>\n",
       "      <td>-66.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-77.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>-87.0</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>413</td>\n",
       "      <td>1160</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4722 rows × 124 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       AP_1   AP_2   AP_3   AP_4   AP_5   AP_6  AP_7   AP_8   AP_9  AP_10  \\\n",
       "0    -100.0 -100.0 -100.0 -100.0 -100.0 -100.0 -74.0 -100.0 -100.0 -100.0   \n",
       "1    -100.0 -100.0 -100.0 -100.0 -100.0 -100.0 -74.0 -100.0 -100.0 -100.0   \n",
       "2    -100.0 -100.0 -100.0 -100.0 -100.0 -100.0 -70.0 -100.0 -100.0 -100.0   \n",
       "3    -100.0 -100.0 -100.0 -100.0 -100.0 -100.0 -70.0 -100.0 -100.0 -100.0   \n",
       "4    -100.0 -100.0 -100.0 -100.0 -100.0 -100.0 -66.0 -100.0 -100.0 -100.0   \n",
       "...     ...    ...    ...    ...    ...    ...   ...    ...    ...    ...   \n",
       "4717 -100.0 -100.0 -100.0 -100.0 -100.0 -100.0 -71.0 -100.0 -100.0 -100.0   \n",
       "4718 -100.0 -100.0 -100.0 -100.0 -100.0 -100.0 -76.0 -100.0 -100.0 -100.0   \n",
       "4719 -100.0 -100.0 -100.0 -100.0 -100.0 -100.0 -76.0 -100.0 -100.0 -100.0   \n",
       "4720  -66.0 -100.0 -100.0 -100.0 -100.0 -100.0 -77.0 -100.0 -100.0 -100.0   \n",
       "4721  -66.0 -100.0 -100.0 -100.0 -100.0 -100.0 -77.0 -100.0 -100.0 -100.0   \n",
       "\n",
       "      ...  AP_115  AP_116  AP_117  AP_118  AP_119  AP_120  AP_121  AP_122  \\\n",
       "0     ...  -100.0  -100.0  -100.0  -100.0  -100.0  -100.0  -100.0  -100.0   \n",
       "1     ...  -100.0  -100.0  -100.0  -100.0  -100.0  -100.0  -100.0  -100.0   \n",
       "2     ...  -100.0  -100.0  -100.0  -100.0  -100.0  -100.0  -100.0  -100.0   \n",
       "3     ...  -100.0  -100.0  -100.0  -100.0  -100.0  -100.0  -100.0  -100.0   \n",
       "4     ...  -100.0  -100.0  -100.0  -100.0  -100.0  -100.0  -100.0  -100.0   \n",
       "...   ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "4717  ...  -100.0  -100.0  -100.0  -100.0  -100.0  -100.0   -84.0  -100.0   \n",
       "4718  ...  -100.0  -100.0  -100.0  -100.0  -100.0  -100.0   -84.0  -100.0   \n",
       "4719  ...  -100.0  -100.0  -100.0  -100.0  -100.0  -100.0   -84.0  -100.0   \n",
       "4720  ...  -100.0  -100.0  -100.0  -100.0  -100.0  -100.0   -87.0  -100.0   \n",
       "4721  ...  -100.0  -100.0  -100.0  -100.0  -100.0  -100.0   -87.0  -100.0   \n",
       "\n",
       "         x     y  \n",
       "0     1173   670  \n",
       "1     1173   670  \n",
       "2     1173   670  \n",
       "3     1173   670  \n",
       "4     1173   670  \n",
       "...    ...   ...  \n",
       "4717   413  1160  \n",
       "4718   413  1160  \n",
       "4719   413  1160  \n",
       "4720   413  1160  \n",
       "4721   413  1160  \n",
       "\n",
       "[4722 rows x 124 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace all -100 in data with 100\n",
    "data.replace(-100, 100, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "scans = data.iloc[:, :-2]\n",
    "coords = data.iloc[:, -2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122\n",
      "estimated_x: 0.0, estimated_y: 0.0, sum_of_weights: 0.0\n",
      "x_coords: [], y_coords: [], weights: []\n",
      "estimated_x: 0.0, estimated_y: 0.0, sum_of_weights: 0.0\n",
      "x_coords: [], y_coords: [], weights: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1384820/2451752849.py:37: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  return estimated_x/sum_of_weights , estimated_y/sum_of_weights\n",
      "/tmp/ipykernel_1384820/2451752849.py:37: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  return estimated_x/sum_of_weights , estimated_y/sum_of_weights\n"
     ]
    }
   ],
   "source": [
    "# perform ap location estimation on custom dataset\n",
    "\n",
    "def get_row_indices_and_signals(scans):\n",
    "    \n",
    "    all_ap_points, all_ap_signals = [], []\n",
    "    for colname in scans.columns:\n",
    "        idx = scans[colname].index[scans[colname] != 100].to_list()\n",
    "        all_ap_points.append(idx)\n",
    "        all_ap_signals.append(scans[colname].loc[idx].to_list())\n",
    "    return all_ap_points, all_ap_signals\n",
    "\n",
    "def get_coordinates_of_heard_scans(indices, coords):\n",
    "    # get the heard ap indices\n",
    "    coordinates = coords.loc[indices]\n",
    "    # get the coordinates of the heard aps\n",
    "    return coordinates[\"x\"].to_numpy(), coordinates[\"y\"].to_numpy()\n",
    "\n",
    "\n",
    "def get_ap_weight(rss):\n",
    "    return 100**(rss/10.0)\n",
    "\n",
    "def approximate_ap_coordinates(x_coords, y_coords, weights):\n",
    "    # sum product of weight and coordinate\n",
    "    estimated_x = np.sum(x_coords * weights)\n",
    "    estimated_y = np.sum(y_coords * weights)\n",
    "    # get denominator\n",
    "    sum_of_weights = weights.sum()\n",
    "    with np.errstate(invalid='raise'):\n",
    "        try:\n",
    "            normalized_x = estimated_x/sum_of_weights\n",
    "            normalized_y = estimated_y/sum_of_weights\n",
    "        except FloatingPointError:\n",
    "            print(f\"estimated_x: {estimated_x}, estimated_y: {estimated_y}, sum_of_weights: {sum_of_weights}\")\n",
    "            print(f\"x_coords: {x_coords}, y_coords: {y_coords}, weights: {weights}\")\n",
    "    # print(normalized_x, normalized_y)\n",
    "    # return tuple of coordinates\n",
    "    return estimated_x/sum_of_weights , estimated_y/sum_of_weights\n",
    "\n",
    "def approximate_all_ap_locations(data, coords):\n",
    "    # get location and signals\n",
    "    all_ap_points, all_ap_signals = get_row_indices_and_signals(data)\n",
    "    estimated_ap_locs = []\n",
    "    # check the length of them both are the same\n",
    "    assert len(all_ap_points) == len(all_ap_signals)\n",
    "    print(len(all_ap_points))\n",
    "    for i in range(len(all_ap_points)):\n",
    "        # get ap indices and rss strengths\n",
    "        idx, signals = all_ap_points[i], all_ap_signals[i]\n",
    "        # get coordinates of the scans\n",
    "        xs, ys = get_coordinates_of_heard_scans(idx, coords)\n",
    "        # get weights\n",
    "        weights = np.asarray([get_ap_weight(signal) for signal in signals])\n",
    "        # get estimated coordinates\n",
    "        estimated_coords = approximate_ap_coordinates(xs, ys, weights)\n",
    "        estimated_ap_locs.append(list(estimated_coords))\n",
    "    return np.asarray(estimated_ap_locs)\n",
    "\n",
    "estimated_ap_locs = approximate_all_ap_locations(scans, coords)\n",
    "# print(len(estimated_ap_locs))\n",
    "# perform ap location estimation on custom dataset\n",
    "# estimated_ap_locs = approximate_all_ap_locations(scans, coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get indices of nans\n",
    "nan_indices = np.argwhere(np.isnan(estimated_ap_locs)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop nans from estimated_ap_locs\n",
    "estimated_ap_locs = estimated_ap_locs[~np.isnan(estimated_ap_locs).any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for indices 29 and 85\n",
    "import numpy as np\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# Assuming 'estimated_ap_locs' is your numpy array with shape (122, 2)\n",
    "gmm = GaussianMixture(n_components=2)\n",
    "gmm.fit(estimated_ap_locs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 756.90913,  893.84983],\n",
       "       [1263.03752, 1004.05223]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert new samples at index 29 and 85 in estimated_ap_locs\n",
    "estimated_ap_locs = np.insert(estimated_ap_locs, 29, new_samples[0], axis=0)\n",
    "estimated_ap_locs = np.insert(estimated_ap_locs, 85, new_samples[1], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put estimated_ap_locs in a dataframe and save it\n",
    "\n",
    "save_loc = \"../data/raw/\"\n",
    "dataset_name = \"UniversityTrain\"\n",
    "ap_coords_df = pd.DataFrame(estimated_ap_locs, columns=[\"x\", \"y\"])\n",
    "\n",
    "# save ap_coords_df\n",
    "ap_coords_df.to_csv(save_loc + dataset_name + \"_ap_coords.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# augment coords and scans using threshold dropping and random dropping\n",
    "from tqdm import tqdm\n",
    "num_aps  = 122\n",
    "# augment dataset with more data\n",
    "def threshold_drop(input, threshold=-90):\n",
    "    cand_indices = []\n",
    "    cand_signals = []\n",
    "    for idx, signal in enumerate(input):\n",
    "        if signal < threshold and signal != 100:\n",
    "            cand_indices.append(idx)\n",
    "            cand_signals.append(signal)\n",
    "    \n",
    "    # get random combintion of random length to drop \n",
    "    rand_nums = []\n",
    "    for i in range(0, 3):\n",
    "        if len(cand_indices) > 0:\n",
    "            rand_nums.append(random.randint(1, len(cand_indices)))\n",
    "\n",
    "    # for each random length, get a random combination of indices to drop\n",
    "    augmented_data = []\n",
    "    for rand_num in rand_nums:\n",
    "        drop_indices = random.sample(cand_indices, rand_num)\n",
    "        # make copy of input\n",
    "        copy = np.copy(input)\n",
    "        # drop indices\n",
    "        copy[drop_indices] = 100\n",
    "        # append to augmented data\n",
    "        augmented_data.append(copy)\n",
    "    \n",
    "    return augmented_data\n",
    "\n",
    "def random_drop(input):\n",
    "    # get several random binary masks of same length as input\n",
    "    masks = []\n",
    "    for i in range(0, 3):\n",
    "        masks.append(np.random.randint(2, size=len(input)))\n",
    "    \n",
    "    # for each mask, drop the indices\n",
    "    augmented_data = []\n",
    "    for mask in masks:\n",
    "        copy = np.copy(input)\n",
    "        copy[mask == 1] = 100\n",
    "        augmented_data.append(copy)\n",
    "    \n",
    "    return augmented_data\n",
    "\n",
    "def augment_dataset(dataset, y, threshold=-90):\n",
    "\n",
    "    # check if dataset is pandas dataframe\n",
    "    if isinstance(dataset, pd.DataFrame):\n",
    "        # change to numpy\n",
    "        dataset = dataset.to_numpy()\n",
    "    # check if y is pandas dataframe or series\n",
    "    if isinstance(y, pd.DataFrame) or isinstance(y, pd.Series):\n",
    "        # change to numpy\n",
    "        y = y.to_numpy()\n",
    "    \n",
    "    new_x = []\n",
    "    new_y = []\n",
    "    # create tqdm loop\n",
    "    loop = tqdm(dataset, total=len(dataset), leave=False)\n",
    "    for idx, scan in enumerate(loop):\n",
    "        # augment scan using  threshold dropper\n",
    "        augmented_thresh = threshold_drop(scan, threshold=threshold)\n",
    "        # augment scan using random dropper\n",
    "        augmented_random = random_drop(scan)\n",
    "        # append to new_x\n",
    "        new_x.append(scan)\n",
    "        new_x.extend(augmented_thresh)\n",
    "        new_x.extend(augmented_random)\n",
    "        # append to new_y\n",
    "        new_y.append(y[idx])\n",
    "        new_y.extend([y[idx]] * len(augmented_thresh))\n",
    "        new_y.extend([y[idx]] * len(augmented_random))\n",
    "    \n",
    "    return np.asarray(new_x), np.asarray(new_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot distribution of values that are not 100 in scans dataframe\n",
    "import matplotlib.pyplot as plt\n",
    "# Define the certain value\n",
    "\n",
    "\n",
    "\n",
    "# Plot the distribution\n",
    "plt.figure(figsize=(8, 6))\n",
    "scans.plot(kind='hist', alpha=0.7, bins=50, edgecolor='black')\n",
    "plt.title(\"Distribution of Values not equal to {}\".format(certain_value))\n",
    "plt.xlabel(\"Value\")\n",
    "# set xlimit to be from -100 to 0\n",
    "plt.xlim(-100, 0)\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    }
   ],
   "source": [
    "augmented_x, augmented_y = augment_dataset(scans, coords, -90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new dataframe from augmented x and y and drop druplicates but keep first\n",
    "augmented_df = pd.concat([pd.DataFrame(augmented_x, columns=[f\"AP_{i}\" for i in range(1, num_aps+1)]), pd.DataFrame(augmented_y, columns=[\"x\", \"y\"])], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicates in augmented dataframe but keep first\n",
    "augmented_df = augmented_df.drop_duplicates(keep='first').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(269, 851), (1770, 679), (1173, 670), (2991, 356), (2780, 1192)]\n"
     ]
    }
   ],
   "source": [
    "# split into train and validation splits, 29 data points in training set, remove 5 for validation\n",
    "\n",
    "# # randomly sample 5 points from training set \n",
    "# random.seed(42)\n",
    "# validation_set = random.sample(training_set, 5)\n",
    "# # change strings to ints\n",
    "# validation_set = [(int(a), int(b)) for a, b in validation_set]\n",
    "# validation_points = [list(a) for a in validation_set]\n",
    "# print(validation_set)\n",
    "# extract the data from the augmented dataset \n",
    "\n",
    "# validation_df = pd.DataFrame(validation_points, columns=[\"x\", \"y\"])\n",
    "\n",
    "\n",
    "# # get validation_data \n",
    "# validation_df = pd.merge(augmented_df, validation_df, on=[\"x\", \"y\"], how=\"inner\")\n",
    "\n",
    "# # filter the data from the augmented dataset # augmented_df = \n",
    "# train_df = augmented_df[~augmented_df.index.isin(validation_df.index)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split 80/20 train/test\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(augmented_df.iloc[:, :-2], augmented_df.iloc[:, -2:], test_size=0.2, random_state=42)\n",
    "\n",
    "train_df = pd.concat([X_train, y_train], axis=1)\n",
    "validation_df = pd.concat([X_valid, y_valid], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide x and y columns in train and validation df by 100 after changing to float\n",
    "\n",
    "train_df['x'] = train_df['x'].astype(float)/100\n",
    "train_df['y'] = train_df['y'].astype(float)/100\n",
    "validation_df['x'] = validation_df['x'].astype(float)/100\n",
    "validation_df['y'] = validation_df['y'].astype(float)/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load estimated ap locs and divide x and y by 100 after making them floats\n",
    "save_loc = \"../data/raw/\"\n",
    "train_name = \"UniversityTrain\"\n",
    "valid_name = \"UniversityValid\"\n",
    "ap_coords_df = pd.DataFrame(estimated_ap_locs, columns=[\"x\", \"y\"])\n",
    "ap_coords_df[\"x\"] = ap_coords_df[\"x\"].astype(float) / 100\n",
    "ap_coords_df[\"y\"] = ap_coords_df[\"y\"].astype(float) / 100\n",
    "# save ap_coords_df\n",
    "ap_coords_df.to_csv(save_loc + train_name + \"_ap_coords.csv\", index=False)\n",
    "ap_coords_df.to_csv(save_loc + valid_name + \"_ap_coords.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save scans and scan coords \n",
    "\n",
    "validation_df.iloc[:, :-2].to_csv(save_loc+valid_name+\"_scans.csv\", index=False)\n",
    "validation_df.iloc[:, -2:].to_csv(save_loc+valid_name+\"_scan_coords.csv\", index=False)\n",
    "\n",
    "train_df.iloc[:, :-2].to_csv(save_loc+train_name+\"_scans.csv\", index=False)\n",
    "train_df.iloc[:, -2:].to_csv(save_loc+train_name+\"_scan_coords.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "envs_dir = \"../customdata/envs2-3/\"\n",
    "\n",
    "env2_scans = pd.read_csv(envs_dir+\"env2.csv\")\n",
    "env2_scans = env2_scans.rename(columns={\"X\": \"x\", \"Y\": \"y\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "env2_scans, env2_coords = env2_scans.iloc[:, :-2], env2_scans.iloc[:, -2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "env2_ap_coords = approximate_all_ap_locations(env2_scans, env2_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make it into dataframe\n",
    "env2_estimated_ap_coords = pd.DataFrame(env2_ap_coords, columns=['x', 'y'])\n",
    "\n",
    "env2_estimated_ap_coords /=100\n",
    "\n",
    "# save to csv \n",
    "save_loc = \"../data/raw/\"\n",
    "name = \"env2\"\n",
    "\n",
    "env2_estimated_ap_coords.to_csv(os.path.join(save_loc, f\"{name}_ap_coords.csv\"), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/14122 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    }
   ],
   "source": [
    "# augment scans and coords\n",
    "augmented_env2x, augmented_env2y = augment_dataset(env2_scans, env2_coords, -90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_env2 = pd.concat([pd.DataFrame(augmented_env2x, columns=env2_scans.columns), pd.DataFrame(augmented_env2y, columns=env2_coords.columns)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicates but keep first \n",
    "augmented_env2 = augmented_env2.drop_duplicates(keep='first').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_env2[\"x\"] = augmented_env2[\"x\"] /100.0\n",
    "augmented_env2[\"y\"] = augmented_env2[\"y\"] /100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split augmented_env2 into train and validation\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(augmented_env2.iloc[:, :-2], augmented_env2.iloc[:, -2:], test_size=0.2, random_state=42)\n",
    "\n",
    "train_df = pd.concat([X_train, y_train], axis=1)\n",
    "validation_df = pd.concat([X_valid, y_valid], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save train and validation dfs\n",
    "\n",
    "train_name = \"env2train\"\n",
    "val_name = \"env2val\"\n",
    "\n",
    "train_df.iloc[:, :-2].to_csv(os.path.join(save_loc, f\"{train_name}_scans.csv\"), index=False)\n",
    "train_df.iloc[:, -2:].to_csv(os.path.join(save_loc, f\"{train_name}_scan_coords.csv\"), index=False)\n",
    "\n",
    "validation_df.iloc[:, :-2].to_csv(os.path.join(save_loc, f\"{val_name}_scans.csv\"), index=False)\n",
    "validation_df.iloc[:, -2:].to_csv(os.path.join(save_loc, f\"{val_name}_scan_coords.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the same with envrionment 3\n",
    "env3_scans = pd.read_csv(envs_dir+\"env3.csv\")\n",
    "env3_scans = env3_scans.rename(columns={\"X\": \"x\", \"Y\": \"y\"})\n",
    "\n",
    "def augment_data(df, name, save_loc=\"../data/raw/\", threshold=-90, divide=True, augment=False):\n",
    "    df_scans, df_coords = df.iloc[:, :-2], df.iloc[:, -2:]\n",
    "\n",
    "    if augment:\n",
    "        augmented_dfx, augmented_dfy = augment_dataset(df_scans, df_coords, threshold)\n",
    "\n",
    "        # make them into dataframes and concat\n",
    "        augmented_df = pd.concat([pd.DataFrame(augmented_dfx, columns=df_scans.columns), pd.DataFrame(augmented_dfy, columns=df_coords.columns)], axis=1)\n",
    "\n",
    "        # drop duplicates\n",
    "        augmented_df = augmented_df.drop_duplicates(keep='first').reset_index(drop=True)\n",
    "    else:\n",
    "        augmented_df = pd.concat([df_scans, df_coords], axis=1)\n",
    "    # divide x and y by 100 to turn to meters\n",
    "    if divide:\n",
    "        augmented_df.x = augmented_df.x.astype(float)\n",
    "        augmented_df.y = augmented_df.y.astype(float)\n",
    "        augmented_df.x /= 100.0\n",
    "        augmented_df.y /= 100.0\n",
    "    \n",
    "    # split to train and test\n",
    "    train_df, test_df = train_test_split(augmented_df, test_size=0.2, random_state=42)\n",
    "\n",
    "    # save to csv\n",
    "    train_name = name + \"train\"\n",
    "    val_name = name + \"val\"\n",
    "\n",
    "    train_df.iloc[:, :-2].to_csv(os.path.join(save_loc, f\"{train_name}_scans.csv\"), index=False)\n",
    "    train_df.iloc[:, -2:].to_csv(os.path.join(save_loc, f\"{train_name}_scan_coords.csv\"), index=False)\n",
    "\n",
    "    test_df.iloc[:, :-2].to_csv(os.path.join(save_loc, f\"{val_name}_scans.csv\"), index=False)\n",
    "    test_df.iloc[:, -2:].to_csv(os.path.join(save_loc, f\"{val_name}_scan_coords.csv\"), index=False)\n",
    "\n",
    "    return train_df, test_df\n",
    "\n",
    "augment_data(env3_scans, \"env3\")\n",
    "augment_data(env2_scans, \"env2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = augmented_y /100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              x         y\n",
      "0      0.284994  0.346623\n",
      "1      0.284994  0.346623\n",
      "2      0.284994  0.346623\n",
      "3      0.284994  0.346623\n",
      "4      0.284994  0.346623\n",
      "...         ...       ...\n",
      "20698  0.045397  0.745321\n",
      "20699  0.045397  0.745321\n",
      "20700  0.045397  0.745321\n",
      "20701  0.045397  0.745321\n",
      "20702  0.045397  0.745321\n",
      "\n",
      "[20703 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "test = augmented_y /100.0\n",
    "# make test a dataframe\n",
    "test = pd.DataFrame(test, columns=[\"x\", \"y\"])\n",
    "scaler = MinMaxScaler()\n",
    "test = pd.DataFrame(scaler.fit_transform(test), columns = test.columns)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.69, 2.44])"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.data_min_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-99.0\n",
      "-9.0\n"
     ]
    }
   ],
   "source": [
    "test2 = pd.DataFrame(augmented_x)\n",
    "overallmin = test2.min().min()\n",
    "overallmax = test2[test2 != 100].max().max()\n",
    "print(overallmin)\n",
    "print(overallmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GNN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
